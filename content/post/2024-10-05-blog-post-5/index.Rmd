---
title: Blog Post 5
author: Victor E. Bowker
date: '2024-10-05'
slug: blog-post-5
categories: []
tags: []
---

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org/"))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Load libraries.
## install via `install.packages("name")`
install.packages("stargazer")
library(car)
library(caret)
library(CVXR)
library(foreign)
library(glmnet)
library(haven)
library(janitor)
library(kableExtra)
library(maps)
library(mlr3)
library(randomForest)
library(ranger)
library(RColorBrewer)
library(sf)
library(tidyverse)
library(viridis)
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
####----------------------------------------------------------#
#### Read, merge, and process data.
####----------------------------------------------------------#

# Read popular vote datasets. 
d_popvote <- read_csv("popvote_1948_2020.csv")
d_state_popvote <- read_csv("state_popvote_1948_2020.csv")
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Read elector distribution dataset. 
d_ec <- read_csv("corrected_ec_1948_2024.csv")
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Read and merge demographics data. 
d_demos <- read_csv("demographics.csv")[,-1]
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Read primary turnout data. 
d_turnout <- read_csv("turnout_1789_2020.csv")
d_state_turnout <- read_csv("state_turnout_1980_2022.csv")
d_state_turnout <- d_state_turnout |> 
  mutate(vep_turnout = as.numeric(str_remove(vep_turnout, "%"))/100) |> 
  select(year, state, vep_turnout)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Read polling data. 
d_polls <- read_csv("national_polls_1968-2024.csv")
d_state_polls <- read_csv("state_polls_1968-2024.csv")
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Process state-level polling data. 
d_pollav_state <- d_state_polls |> 
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support, na.rm = TRUE)) |>
  top_n(1, poll_date) |> 
  rename(latest_pollav = poll_support) |>
  select(-c(weeks_left, days_left, poll_date, candidate, before_convention)) |>
  pivot_wider(names_from = party, values_from = c(latest_pollav, mean_pollav))
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Read processed ANES data. 
anes <- read_dta("anes_timeseries_cdf_stata_20220916.dta") # Total ANES Cumulative Data File. 
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
anes <- anes |> 
  mutate(year = VCF0004,
         pres_vote = case_when(VCF0704a == 1 ~ 1, 
                               VCF0704a == 2 ~ 2, 
                               .default = NA), 
         # Demographics
         age = VCF0101, 
         gender = VCF0104, # 1 = Male; 2 = Female; 3 = Other
         race = VCF0105b, # 1 = White non-Hispanic; 2 = Black non-Hispanic, 3 == Hispanic; 4 = Other or multiple races, non-Hispanic; 9 = missing/DK
         educ = VCF0110, # 0 = DK; 1 = Less than high school; 2. High school; 3 = Some college; 4 = College+ 
         income = VCF0114, # 1 = 0-16 percentile; 2 = 17-33 percentile; 3 = 34-67; 4 = 68 to 95; 5 = 96 to 100. 
         religion = VCF0128, # 0 = DK; 1 = Protestant; 2 = Catholic; 3 = Jewish; 4 = Other
         attend_church = case_when(
           VCF0004 < 1972 ~ as.double(as.character(VCF0131)),
           TRUE ~ as.double(as.character(VCF0130))
         ), # 1 = every week - regularly; 2 = almost every week - often; 3 = once or twice a month; 4 = a few times a year - seldom; 5 = never ; 6 = no religious preference
         southern = VCF0113,
         region = VCF0113, 
         work_status = VCF0118,
         homeowner = VCF0146, 
         married = VCF0147,
        
         # 7-point PID
         pid7 = VCF0301, # 0 = DK; 1 = Strong Democrat; 2 = Weak Democrat; 3 = Independent - Democrat; 4 = Independent - Independent; 5 = Independent - Republican; 6 = Weak Republican; 7 = Strong Republican
         
         # 3-point PID
         pid3 = VCF0303, # 0 = DK; 1 = Democrats; 2 = Independents; 3 = Republicans. 
         
         # 3-point ideology. 
         ideo = VCF0804 # 0, 9 = DK; 1 = Liberal; 2 = Moderate; 3 = Conservative
         ) |> 
  select(year, pres_vote, age, gender, race, educ, income, religion, attend_church, southern, 
         region, work_status, homeowner, married, pid7, pid3, ideo)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# How well do demographics predict vote choice? 
anes_year <- anes[anes$year == 2016,] |> 
  select(-c(year, pid7, pid3, ideo)) |>
  mutate(pres_vote = factor(pres_vote, levels = c(1, 2), labels = c("Democrat", "Republican"))) |> 
  filter(!is.na(pres_vote)) |>
  clean_names()

n_features <- length(setdiff(names(anes_year), "pres_vote"))

set.seed(02138)
train.ind <- createDataPartition(anes_year$pres_vote, p = 0.8, list = FALSE)

anes_train <- anes_year[train.ind,]
anes_test <- anes_year[-train.ind,]
```


# Welcome Back

This week is _all about_ demographics, so lets talk about it!

When pollsters review data to make predictions, they have _a lot_ of factors to consider. I have covered some of these in prior posts, including economic factors, public opinion polls, and historical trends. Today, I will explore trends that are impacted--or are not--by demographics. The primary demographics included in today's data are race, gender, education level, income, religion, and a few others!

For the first regression of the way (woot woot we love regressions) take a look down below. This regression tells us which demographic factors have been historically most accurate, and which demonstrated less accurate results.

Interestingly, this form shows that age is not statistically significant in which party a candidate votes for. I say this is interesting because the Harvard Kennedy School Institute of Politics Spring 2024 Poll demonstrated that likely young voters were much more likely to vote for the Democrat (Biden) compared to the Republican (Trump). For voters under 30, Biden had the lead of 8 percentage points, which seemingly demonstrates something different than this-- _interesting_![1]

Additionally, I see that gender and race are in fact quite demonstrative of voting patterns, with negative coefficients of -0.429 and -0.548 respectively. According to the Pew Research Center, data shows that black voters turnout less than white voters, but overwhelmingly vote for Democrats, with roughly 93% of black voters voting for Democrats in the 2022 midterm elections![2]

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Logistic regression. 
logit_fit <- glm(pres_vote ~ ., 
                 family = "binomial", 
                 data = anes_train)
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}

# In-sample goodness-of-fit. 
summary(logit_fit)

```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
library(stargazer)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE}
# Logistic regression
logit_fit <- glm(pres_vote ~ ., family = "binomial", data = anes_train)

# Generate a nice summary table using stargazer
stargazer(logit_fit, type = "text", title = "Logistic Regression Results")
```


# Moving On :)

Next, take a look at this in-sample accuracy analysis, which will help us understand how succesful the regression model is in predicting the vote outcome!

You will see first that the True Positives (positive meaning Democrat) came to 746, while True Negative (Republican) is 660. False positives (meaning they incorrectly guessed Republican) is 336, and the False negative is 346. These equate to a accuracy of 67.34%...which to me doesn't sound great. If we can only predict the outcome of the election correctly 67% of the time, what are we even here for!? (just kidding on the last part of course!) This model is useful, but not perfect!

You will see an out of sample response below, and spoiler alert, it is not much different!


```{r, echo=FALSE, warning = FALSE, message = FALSE}
# In-sample accuracy.
logit.is <- factor(ifelse(predict(logit_fit, type = "response") > 0.5, 2, 1), 
                   levels = c(1, 2), labels = c("Democrat", "Republican"))
(cm.rf.logit.is <- confusionMatrix(logit.is, anes_train$pres_vote))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE}
# Out-of-sample accuracy. 
logit_pred <- factor(ifelse(predict(logit_fit, anes_test, type = "response") > 0.5, 2, 1), 
                     levels = c(1, 2), labels = c("Democrat", "Republican"))
(cm.rf.logit.oos <- confusionMatrix(logit_pred, anes_test$pres_vote))


```

# Other Avenues

So, what about random forest? How accurate are random forest models in this projection? Take a look! Also, in case you were wondering, the Random Forest model is an ensemble learning tool that constructs decision trees, and then when it runs regressions or classifications it outputs the results of individual trees! Random forests are fun-and good for the environment (actually probably not considering the power used to run my computer, and the server, and github...but then again, this is a gov class, not a ESPP one!)

In the random forest model you can see that the accuracy comes out to just above 70%, meaning it is the most accurate so far!

```{r, echo=FALSE, warning = FALSE, message = FALSE}
# Random forest: 
rf_fit <- ranger(pres_vote ~ ., 
                 mtry = floor(n_features/3), 
                 respect.unordered.factors = "order", 
                 seed <- 02138,
                 classification = TRUE,
                 data = anes_train)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE}
# In-sample accuracy.
(cm.rf.is <- confusionMatrix(rf_fit$predictions, anes_train$pres_vote))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Out-of-sample accuracy. 
rf_pred <- predict(rf_fit, data = anes_test)
(cm.rf.oos <- confusionMatrix(rf_pred$predictions, anes_test$pres_vote))
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Read and merge 1% voterfile data into one dataset. 
voterfile.sample.files <- list.files("state_1pc_samples_aug24")

voterfile.complete <- read_csv(paste0("state_1pc_samples_aug24/", voterfile.sample.files[1]))
for (i in 2:length(voterfile.sample.files)) {
  temp <- read_csv(paste0("state_1pc_samples_aug24/",voterfile.sample.files[i]))
  voterfile.complete <- rbind(voterfile.complete, temp)
}
```


# What does data look like?

I  often look at sites like 538 or CNN to view election projections and wonder what their data is, and where it comes from. There is so much available information in the world, not even including the data behind paywalls or copyrights. Just as an example, I have attached a tidbit of Massachusetts voter file information, which includes variables of age, age range, and gender, as well as if the voter is deceased or not. 

As a Massachusetts resident, perhaps you could find my information in this data set, but I can confidently say that this small preview does not include me--considering there are no gender and age matches for me!

Now, scroll on to see what I do with this data!

```{r, echo=FALSE, warning = FALSE, message = FALSE}
# Mass example. 
vf_ma <- read_csv("state_1pc_samples_aug24/MA_sample.csv")

vf_ma_1 <- vf_ma |>
  select(1:5)

head(vf_ma_1, 10)

```


```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Merge data.
d <- d_pollav_state |> 
  left_join(d_state_popvote, by = c("year", "state")) |>  
  left_join(d_popvote |> filter(party == "democrat"), by = "year") |> 
  left_join(d_demos, by = c("year", "state")) |> 
  left_join(d_state_turnout, by = c("year", "state")) |> 
  filter(year >= 1980) |> 
  ungroup()
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Sequester states for which we have polling data for 2024. 
states.2024 <- unique(d$state[d$year == 2024])
states.2024 <- states.2024[-which(states.2024 == "Nebraska Cd 2")]
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Subset and split data.
d <- d |> 
  filter(state %in% states.2024)

d_train <- d |> 
  filter(year < 2024)
d_test <- d |> 
  filter(year == 2024)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Example pooled model with turnout and demographics. 
mod_lm_dem <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + vep_turnout + total_pop + white + black + american_indian + 
                 asian_pacific_islander + other_race + two_or_more_races + hispanic_white +
                 less_than_college + bachelors + graduate + incumbent + incumbent_party, 
                 data = d_train)
summary(mod_lm_dem)
mod_lm_rep <- lm(R_pv2p ~ R_pv2p_lag1 + R_pv2p_lag2 + latest_pollav_REP + mean_pollav_REP + vep_turnout + total_pop + white + black + american_indian + 
                 asian_pacific_islander + other_race + two_or_more_races + hispanic_white +
                   less_than_college + bachelors + graduate,
                 data = d_train)
summary(mod_lm_rep)
```

# Simulation Time!

This week in lab, we learned how simulations work--and how sometimes maybe they don't work. Luckily, this one seems to work. After taking a little while to run on my computer (10,000 simulations!!), we have results to examine!

 
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Simple simulation example: 
simp.vars <- c("D_pv2p_lag1", "D_pv2p_lag2", "latest_pollav_DEM", "mean_pollav_DEM",
               "R_pv2p_lag1", "R_pv2p_lag2", "latest_pollav_REP", "mean_pollav_REP",
               "vep_turnout")
mod_lm_dem_simp <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + vep_turnout,
                      data = d_train)
mod_lm_rep_simp <- lm(R_pv2p ~ R_pv2p_lag1 + R_pv2p_lag2 + latest_pollav_REP + mean_pollav_REP + vep_turnout,
                      data = d_train)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# What data do we have for 2024? 
d_test |> select(all_of(simp.vars)) |> view()
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Add back in lagged vote share for 2024. 
t <- d |> 
  filter(year >= 2016) |> 
  arrange(year) |> 
  group_by(state) |> 
  mutate(
    D_pv2p_lag1 = lag(D_pv2p, 1),
    R_pv2p_lag1 = lag(R_pv2p, 1), 
    D_pv2p_lag2 = lag(D_pv2p, 2),
    R_pv2p_lag2 = lag(R_pv2p, 2)) |> 
  filter(year == 2024) |> 
  select(state, year, D_pv2p, R_pv2p, D_pv2p_lag1, R_pv2p_lag1, D_pv2p_lag2, R_pv2p_lag2) 
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Subset testing data to only relevant variables for our simple model. 
d_test_simp <- d_test |> 
  select(-c(R_pv2p, R_pv2p_lag1, R_pv2p_lag2, 
            D_pv2p, D_pv2p_lag1, D_pv2p_lag2)) |> 
  left_join(t, by = c("state", "year")) |> 
  select(state, year, all_of(simp.vars))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Get average state-level turnout accross 2020, 2016, 2012.  
d_turnout_avg <- d_train |> 
  filter(year %in% c(2020, 2016, 2012)) |> 
  filter(state %in% unique(d_test_simp$state)) |> 
  group_by(state) |> 
  summarize(vep_turnout = mean(vep_turnout, na.rm = TRUE))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Make predictions with simple average turnout. 
d_test_simp <- d_test_simp |> 
  left_join(d_turnout_avg, by = "state") |> 
  select(-vep_turnout.x) |> 
  rename(vep_turnout = vep_turnout.y)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
simp_pred_dem <- predict(mod_lm_dem_simp, d_test_simp)
simp_pred_rep <- predict(mod_lm_rep_simp, d_test_simp)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Create dataset to summarize winners and EC vote distributions. 
win_pred <- data.frame(state = d_test_simp$state,
                       year = rep(2024, length(d_test_simp$state)),
                       simp_pred_dem = simp_pred_dem,
                       simp_pred_rep = simp_pred_rep,
                       winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican")) |>
  left_join(d_ec, by = c("state", "year"))

win_pred |> 
  filter(winner == "Democrat") |> 
  select(state)

win_pred |> 
  filter(winner == "Republican") |> 
  select(state)

win_pred |> 
  group_by(winner) |> 
  summarize(n = n(), ec = sum(electors))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
win_pred
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
us_states <- map_data("state")
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# section written in consultation with ChatGPT
win_pred$state <- tolower(win_pred$state) 
map_data <- us_states |>
  left_join(win_pred, by = c("region" = "state"))
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
projection_graph <- ggplot(map_data, aes(x = long, y = lat, group = group, fill = winner)) +
  geom_polygon(color = "white") +
  coord_fixed(1.3) +
  labs(title = "               2024 Election Map Projection", 
       fill = "Predicted Winner") +
  scale_fill_manual(values = c("Democrat" = "blue4", "Republican" = "firebrick")) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank())
```


```{r, echo=FALSE, warning = FALSE, message = FALSE}
projection_graph
```



```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Now let's simulate this with varying levels of turnout and get both confidence intervals on our predictions
# and approximate win percentages for each state. 
m <- 1e4 # Number of simulations.
pred.mat <- data.frame(state = rep(d_test_simp$state, m),
                       year = rep(2024, m*length(d_test_simp$state)),
                       vep_turnout = rep(d_turnout_avg$vep_turnout, m),
                       simp_pred_dem = rep(simp_pred_dem, m),
                       simp_pred_rep = rep(simp_pred_rep, m))

j <- 1
for (i in 1:m) {
  print(i)
  vep_turnout <- sapply(d_turnout_avg$vep_turnout, function(mu) {
    rnorm(1, mean = mu, sd = 0.05) # Simulate turnout from Gaussian centered on state average with 5% SD.
  })

  d_test_samp <- d_test_simp
  d_test_samp$vep_turnout <- vep_turnout

  simp_pred_dem <- predict(mod_lm_dem_simp, d_test_samp)
  simp_pred_rep <- predict(mod_lm_rep_simp, d_test_samp)

  pred.mat$simp_pred_dem[j:(i*19)] <- simp_pred_dem
  pred.mat$simp_pred_rep[j:(i*19)] <- simp_pred_rep
  j <- j + 19 # Hack for filling out matrix.
}

pred.mat <- pred.mat |>
  mutate(winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican"))

pred.mat |>
  group_by(state, winner) |>
  summarize(win_rate = n()/m) |>
  view()
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Now we can calculate confidence intervals for each state.
pred.mat |>
  group_by(state) |>
  summarize(mean_dem = mean(simp_pred_dem),
            mean_rep = mean(simp_pred_rep),
            sd_dem = sd(simp_pred_dem),
            sd_rep = sd(simp_pred_rep),
            lower_dem = mean_dem - 1.96*sd_dem,
            upper_dem = mean_dem + 1.96*sd_dem,
            lower_rep = mean_rep - 1.96*sd_rep,
            upper_rep = mean_rep + 1.96*sd_rep) |>
  view()
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}

win_summary <- pred.mat |>
  group_by(state) |>
  summarize(mean_dem = mean(simp_pred_dem),
            mean_rep = mean(simp_pred_rep),
            sd_dem = sd(simp_pred_dem),
            sd_rep = sd(simp_pred_rep),
            lower_dem = mean_dem - 1.96 * sd_dem,
            upper_dem = mean_dem + 1.96 * sd_dem,
            lower_rep = mean_rep - 1.96 * sd_rep,
            upper_rep = mean_rep + 1.96 * sd_rep) |>
  ungroup()
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
win_summary
```

```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
install.packages("ggplot2")
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
library(ggplot2)
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
install.packages("tidyverse")
```
```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
library(tidyverse)
```


```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
# Sample win summary data
win_summary <- pred.mat |>
  group_by(state, winner) |>
  summarize(win_rate = n() / m, .groups = "drop")

# Calculate mean predicted values for each party
win_summary <- pred.mat |>
  group_by(state) |>
  summarize(
    mean_dem = mean(simp_pred_dem),
    mean_rep = mean(simp_pred_rep),
    winner = ifelse(mean_dem > mean_rep, "Democrat", "Republican"),
    .groups = "drop"
  )

```


After all that _fun_ wrangling, you will see results below!


```{r, echo=FALSE, warning = FALSE, message = FALSE, include=FALSE}
library(tidyr)

# Reshape to long format
win_long <- win_summary |>
  pivot_longer(
    cols = starts_with("mean_"),
    names_to = "party",
    values_to = "percentage",
    names_prefix = "mean_"
  )

```
```{r, echo=FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
```
```{r}
#small section written in consultation with ChatGPT
ggplot(win_long, aes(x = state, y = percentage, fill = party)) +
  geom_bar(stat = "identity") + 
  labs(title = "Projected Election Outcomes by State (2024)",
       x = "State",
       y = "Projected Win Percentage",
       fill = "Party") +
  theme_minimal() +
  scale_fill_manual(values = c("blue4", "firebrick")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  + 
     geom_hline(yintercept = 50, linetype = "solid", color = "green") 
```
```{r}


```
```{r}


```
Citations
[1] - https://iop.harvard.edu/youth-poll/47th-edition-spring-2024

[2] - https://www.pewresearch.org/politics/2023/07/12/voting-patterns-in-the-2022-elections/

[] - as always, code borrowed from Teaching Fellow Matthew Dardet, check him out here - https://www.matthewdardet.com
